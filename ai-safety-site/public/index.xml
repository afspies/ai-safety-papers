<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Safety Livefeed on AI Safety Papers</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in AI Safety Livefeed on AI Safety Papers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bilinear MLPs enable weight-based mechanistic interpretability</title>
      <link>http://localhost:1313/en/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces an important advancement in neural network interpretability by analyzing bilinear MLPs, which are a variant of Gated Linear Units (GLUs) without element-wise nonlinearity. The authors demonstrate that these bilinear MLPs can achieve competitive performance while being significantly more interpretable than traditional MLPs. Key contributions:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bilinear MLPs enable weight-based mechanistic interpretability</title>
      <link>http://localhost:1313/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces an important advancement in neural network interpretability by analyzing bilinear MLPs, which are a variant of Gated Linear Units (GLUs) without element-wise nonlinearity. The authors demonstrate that these bilinear MLPs can achieve competitive performance while being significantly more interpretable than traditional MLPs. Key contributions:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT</title>
      <link>http://localhost:1313/en/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents a novel approach to discovering interpretable circuits in transformer models using dictionary learning, focusing on a case study of Othello-GPT. The work advances mechanistic interpretability by providing a patch-free alternative to traditional circuit discovery methods. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A new framework for circuit discovery that avoids activation patching and offers better computational efficiency&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A comprehensive analysis of where to apply dictionary learning in transformer architectures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Detailed case studies showing how the method reveals interpretable circuits in Othello-GPT The authors propose decomposing three key components: - Word embeddings - Attention layer outputs - MLP layer outputs&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT</title>
      <link>http://localhost:1313/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents a novel approach to discovering interpretable circuits in transformer models using dictionary learning, focusing on a case study of Othello-GPT. The work advances mechanistic interpretability by providing a patch-free alternative to traditional circuit discovery methods. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A new framework for circuit discovery that avoids activation patching and offers better computational efficiency&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A comprehensive analysis of where to apply dictionary learning in transformer architectures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Detailed case studies showing how the method reveals interpretable circuits in Othello-GPT The authors propose decomposing three key components: - Word embeddings - Attention layer outputs - MLP layer outputs&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Extracting Finite State Machines from Transformers</title>
      <link>http://localhost:1313/en/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper investigates how transformers learn regular languages from a mechanistic interpretability perspective. The authors develop a method to extract finite state machines (Moore machines) from trained transformers and analyze how these models internally represent different states. Key contributions: - Extension of the L* algorithm to extract Moore machines from transformers - Analysis of how transformers represent states in their activation space - Characterization of which regular languages one-layer transformers can learn with good generalization - Identification of failure modes due to attention saturation The authors begin by examining different training tasks for regular languages:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Extracting Finite State Machines from Transformers</title>
      <link>http://localhost:1313/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper investigates how transformers learn regular languages from a mechanistic interpretability perspective. The authors develop a method to extract finite state machines (Moore machines) from trained transformers and analyze how these models internally represent different states. Key contributions: - Extension of the L* algorithm to extract Moore machines from transformers - Analysis of how transformers represent states in their activation space - Characterization of which regular languages one-layer transformers can learn with good generalization - Identification of failure modes due to attention saturation The authors begin by examining different training tasks for regular languages:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to use and interpret activation patching</title>
      <link>http://localhost:1313/en/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper provides a comprehensive guide to activation patching, an important mechanistic interpretability technique for understanding neural networks. The authors, Stefan Heimersheim and Neel Nanda, draw from extensive practical experience to outline best practices and common pitfalls. Key aspects covered include: **&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Core Concepts** - Activation patching involves replacing internal activations of a neural network during execution to understand how different components contribute to model behavior - The technique is more targeted than simple ablation, allowing researchers to isolate specific behaviors and circuits - The authors illustrate the concept with an example of analyzing how a model completes &amp;ldquo;The Colosseum is in&amp;rdquo; → &amp;ldquo;Rome&amp;rdquo;, demonstrating how different patching approaches can isolate language processing vs factual recall **&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to use and interpret activation patching</title>
      <link>http://localhost:1313/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper provides a comprehensive guide to activation patching, an important mechanistic interpretability technique for understanding neural networks. The authors, Stefan Heimersheim and Neel Nanda, draw from extensive practical experience to outline best practices and common pitfalls. Key aspects covered include: **&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Core Concepts** - Activation patching involves replacing internal activations of a neural network during execution to understand how different components contribute to model behavior - The technique is more targeted than simple ablation, allowing researchers to isolate specific behaviors and circuits - The authors illustrate the concept with an example of analyzing how a model completes &amp;ldquo;The Colosseum is in&amp;rdquo; → &amp;ldquo;Rome&amp;rdquo;, demonstrating how different patching approaches can isolate language processing vs factual recall **&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning</title>
      <link>http://localhost:1313/en/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces end-to-end sparse dictionary learning (e2e SAE), a novel method for training sparse autoencoders (SAEs) that improves their ability to identify functionally important features in neural networks. &lt;strong&gt;Key Innovation:&lt;/strong&gt; Traditional SAEs are trained to minimize reconstruction error of network activations, but this approach may learn features that aren&amp;rsquo;t maximally relevant for the network&amp;rsquo;s actual computation. The authors propose training SAEs by minimizing the KL divergence between the output distributions of:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning</title>
      <link>http://localhost:1313/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces end-to-end sparse dictionary learning (e2e SAE), a novel method for training sparse autoencoders (SAEs) that improves their ability to identify functionally important features in neural networks. &lt;strong&gt;Key Innovation:&lt;/strong&gt; Traditional SAEs are trained to minimize reconstruction error of network activations, but this approach may learn features that aren&amp;rsquo;t maximally relevant for the network&amp;rsquo;s actual computation. The authors propose training SAEs by minimizing the KL divergence between the output distributions of:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Interpreting Attention Layer Outputs with Sparse Autoencoders</title>
      <link>http://localhost:1313/en/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents a novel approach to interpreting attention mechanisms in transformer models using Sparse Autoencoders (SAEs). The authors demonstrate that SAEs can effectively decompose attention layer outputs into interpretable features, providing new insights into how attention layers process information.&lt;/p&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Overview. We train Sparse Autoencoders (SAEs) on \ $\textbf{z}_{\text{cat}}\ $ , the attention layer outputs pre-linear, concatenated across all heads. The SAEs extract linear directions that correspond to concepts in the model, giving us insight into what attention layers learn in practice. Further, we uncover what information was used to compute these features with direct feature attribution (DFA, Section 2).&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Overview. We train Sparse Autoencoders (SAEs) on \ $\textbf{z}_{\text{cat}}\ $ , the attention layer outputs pre-linear, concatenated across all heads. The SAEs extract linear directions that correspond to concepts in the model, giving us insight into what attention layers learn in practice. Further, we uncover what information was used to compute these features with direct feature attribution (DFA, Section 2).&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 The key contributions include:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Interpreting Attention Layer Outputs with Sparse Autoencoders</title>
      <link>http://localhost:1313/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents a novel approach to interpreting attention mechanisms in transformer models using Sparse Autoencoders (SAEs). The authors demonstrate that SAEs can effectively decompose attention layer outputs into interpretable features, providing new insights into how attention layers process information.&lt;/p&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Overview. We train Sparse Autoencoders (SAEs) on \ $\textbf{z}_{\text{cat}}\ $ , the attention layer outputs pre-linear, concatenated across all heads. The SAEs extract linear directions that correspond to concepts in the model, giving us insight into what attention layers learn in practice. Further, we uncover what information was used to compute these features with direct feature attribution (DFA, Section 2).&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Overview. We train Sparse Autoencoders (SAEs) on \ $\textbf{z}_{\text{cat}}\ $ , the attention layer outputs pre-linear, concatenated across all heads. The SAEs extract linear directions that correspond to concepts in the model, giving us insight into what attention layers learn in practice. Further, we uncover what information was used to compute these features with direct feature attribution (DFA, Section 2).&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 The key contributions include:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders</title>
      <link>http://localhost:1313/en/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces Llama Scope, a comprehensive suite of 256 Sparse Autoencoders (SAEs) trained on the Llama-3.1-8B language model. The work represents a significant advancement in mechanistic interpretability research by providing ready-to-use, open-source SAE models that can help researchers understand the internal representations of large language models. Key aspects of the work include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Architecture and Training Position&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Four potential training positions in one Transformer Block.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Four potential training positions in one Transformer Block.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 The authors train SAEs at four different positions within each transformer block: - Post-MLP Residual Stream (R) - Attention Output (A) - MLP Output (M) - Transcoder (TC)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders</title>
      <link>http://localhost:1313/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces Llama Scope, a comprehensive suite of 256 Sparse Autoencoders (SAEs) trained on the Llama-3.1-8B language model. The work represents a significant advancement in mechanistic interpretability research by providing ready-to-use, open-source SAE models that can help researchers understand the internal representations of large language models. Key aspects of the work include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Architecture and Training Position&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Four potential training positions in one Transformer Block.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Four potential training positions in one Transformer Block.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 The authors train SAEs at four different positions within each transformer block: - Post-MLP Residual Stream (R) - Attention Output (A) - MLP Output (M) - Transcoder (TC)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Interpretability for AI Safety - A Review</title>
      <link>http://localhost:1313/en/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This comprehensive review paper examines mechanistic interpretability, an emerging approach to understanding the inner workings of AI systems that aims to reverse engineer neural networks into human-understandable algorithms and concepts.&lt;/p&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 The paper begins by contrasting mechanistic interpretability with other interpretability paradigms: - Behavioral: Analyzes input-output relationships - Attributional: Quantifies individual input feature influences - Concept-based: Identifies high-level representations - Mechanistic: Uncovers precise causal mechanisms from inputs to outputs The authors establish several core concepts and hypotheses:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Interpretability for AI Safety - A Review</title>
      <link>http://localhost:1313/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This comprehensive review paper examines mechanistic interpretability, an emerging approach to understanding the inner workings of AI systems that aims to reverse engineer neural networks into human-understandable algorithms and concepts.&lt;/p&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 The paper begins by contrasting mechanistic interpretability with other interpretability paradigms: - Behavioral: Analyzes input-output relationships - Attributional: Quantifies individual input feature influences - Concept-based: Identifies high-level representations - Mechanistic: Uncovers precise causal mechanisms from inputs to outputs The authors establish several core concepts and hypotheses:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Interpretability of Reinforcement Learning Agents</title>
      <link>http://localhost:1313/en/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper explores the mechanistic interpretability of reinforcement learning (RL) agents by analyzing a neural network trained on procedural maze environments. The research focuses on understanding how the network processes information and makes decisions, with particular attention to goal misgeneralization. Key aspects of the study:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Goal Misgeneralization Analysis&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;[See Figure 1 in the original paper]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The researchers examined how RL agents can develop biases in their navigation strategies, particularly the tendency to move toward the top-right corner even when the goal is elsewhere. This figure effectively illustrates different scenarios of goal misgeneralization and robust agent behavior.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Interpretability of Reinforcement Learning Agents</title>
      <link>http://localhost:1313/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper explores the mechanistic interpretability of reinforcement learning (RL) agents by analyzing a neural network trained on procedural maze environments. The research focuses on understanding how the network processes information and makes decisions, with particular attention to goal misgeneralization. Key aspects of the study:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Goal Misgeneralization Analysis&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;[See Figure 1 in the original paper]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The researchers examined how RL agents can develop biases in their navigation strategies, particularly the tendency to move toward the top-right corner even when the goal is elsewhere. This figure effectively illustrates different scenarios of goal misgeneralization and robust agent behavior.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations</title>
      <link>http://localhost:1313/en/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents a comprehensive analysis of non-factual hallucinations in large language models (LMs), identifying two distinct mechanisms through which these errors occur and proposing a novel method for mitigating them. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identification of Two Hallucination Types:&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Our main finding of two non-factual hallucination mechanisms. Left (a): The early-site hallucinations are caused by lacking general knowledge of the subject in lower layer MLPs of transformer LMs – in this case, the model fails to retrieve useful information about the entity (e.g., Orica, an Australian-based multinational corporation) to generate the correct object attribute (e.g., Australia), and therefore outputs a highly irrelevant prediction (January). Right (b): The late-site hallucinations are caused by the failure of upper layer attention heads and MLPs to identify the most relevant object to the given subject and relation – in this case, the model is able to retrieve related information about the subject (e.g., Toulouse, a French city) from lower layer MLPs, but cannot distinguish the irrelevant yet strongly associated attributes (e.g., Paris) from the correct answers (e.g., Bologna/Chongqing/Atlanta). We found that these two types of hallucinations can be distinguished by the relative causal contribution to model predictions between lower and upper layer LM components.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Our main finding of two non-factual hallucination mechanisms. Left (a): The early-site hallucinations are caused by lacking general knowledge of the subject in lower layer MLPs of transformer LMs – in this case, the model fails to retrieve useful information about the entity (e.g., Orica, an Australian-based multinational corporation) to generate the correct object attribute (e.g., Australia), and therefore outputs a highly irrelevant prediction (January). Right (b): The late-site hallucinations are caused by the failure of upper layer attention heads and MLPs to identify the most relevant object to the given subject and relation – in this case, the model is able to retrieve related information about the subject (e.g., Toulouse, a French city) from lower layer MLPs, but cannot distinguish the irrelevant yet strongly associated attributes (e.g., Paris) from the correct answers (e.g., Bologna/Chongqing/Atlanta). We found that these two types of hallucinations can be distinguished by the relative causal contribution to model predictions between lower and upper layer LM components.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 - Knowledge Enrichment Hallucinations: Occur when lower-layer MLPs fail to retrieve sufficient subject attribute knowledge - Answer Extraction Hallucinations: Happen when upper-layer attention heads fail to select the correct object attribute&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations</title>
      <link>http://localhost:1313/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents a comprehensive analysis of non-factual hallucinations in large language models (LMs), identifying two distinct mechanisms through which these errors occur and proposing a novel method for mitigating them. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identification of Two Hallucination Types:&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Our main finding of two non-factual hallucination mechanisms. Left (a): The early-site hallucinations are caused by lacking general knowledge of the subject in lower layer MLPs of transformer LMs – in this case, the model fails to retrieve useful information about the entity (e.g., Orica, an Australian-based multinational corporation) to generate the correct object attribute (e.g., Australia), and therefore outputs a highly irrelevant prediction (January). Right (b): The late-site hallucinations are caused by the failure of upper layer attention heads and MLPs to identify the most relevant object to the given subject and relation – in this case, the model is able to retrieve related information about the subject (e.g., Toulouse, a French city) from lower layer MLPs, but cannot distinguish the irrelevant yet strongly associated attributes (e.g., Paris) from the correct answers (e.g., Bologna/Chongqing/Atlanta). We found that these two types of hallucinations can be distinguished by the relative causal contribution to model predictions between lower and upper layer LM components.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Our main finding of two non-factual hallucination mechanisms. Left (a): The early-site hallucinations are caused by lacking general knowledge of the subject in lower layer MLPs of transformer LMs – in this case, the model fails to retrieve useful information about the entity (e.g., Orica, an Australian-based multinational corporation) to generate the correct object attribute (e.g., Australia), and therefore outputs a highly irrelevant prediction (January). Right (b): The late-site hallucinations are caused by the failure of upper layer attention heads and MLPs to identify the most relevant object to the given subject and relation – in this case, the model is able to retrieve related information about the subject (e.g., Toulouse, a French city) from lower layer MLPs, but cannot distinguish the irrelevant yet strongly associated attributes (e.g., Paris) from the correct answers (e.g., Bologna/Chongqing/Atlanta). We found that these two types of hallucinations can be distinguished by the relative causal contribution to model predictions between lower and upper layer LM components.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}
&lt;/script&gt; 
&lt;p&gt;Figure 1 - Knowledge Enrichment Hallucinations: Occur when lower-layer MLPs fail to retrieve sufficient subject attribute knowledge - Answer Extraction Hallucinations: Happen when upper-layer attention heads fail to select the correct object attribute&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization</title>
      <link>http://localhost:1313/en/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents &amp;ldquo;mechanistic unlearning,&amp;rdquo; a novel approach for robustly removing or editing knowledge in large language models (LLMs) by targeting specific model components identified through mechanistic interpretability. Key Contributions: - Introduces a more precise and effective method for knowledge editing/unlearning that focuses on modifying the &amp;ldquo;fact lookup&amp;rdquo; (FLU) mechanism rather than just output-focused components - Demonstrates superior robustness against prompt variations and relearning attempts compared to existing methods - Provides evidence that targeting FLU mechanisms leads to better disruption of latent knowledge while maintaining general model capabilities&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization</title>
      <link>http://localhost:1313/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper presents &amp;ldquo;mechanistic unlearning,&amp;rdquo; a novel approach for robustly removing or editing knowledge in large language models (LLMs) by targeting specific model components identified through mechanistic interpretability. Key Contributions: - Introduces a more precise and effective method for knowledge editing/unlearning that focuses on modifying the &amp;ldquo;fact lookup&amp;rdquo; (FLU) mechanism rather than just output-focused components - Demonstrates superior robustness against prompt variations and relearning attempts compared to existing methods - Provides evidence that targeting FLU mechanisms leads to better disruption of latent knowledge while maintaining general model capabilities&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Scalable Mechanistic Neural Networks</title>
      <link>http://localhost:1313/en/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces the Scalable Mechanistic Neural Network (S-MNN), an enhanced version of the original Mechanistic Neural Network (MNN) that significantly improves computational efficiency while maintaining accuracy. The key innovation is reducing both time and space complexities from cubic/quadratic to linear with respect to sequence length. &lt;strong&gt;Key Contributions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Complexity Reduction&lt;/strong&gt; - Reformulated the original MNN&amp;rsquo;s underlying linear system by eliminating slack variables and central difference constraints - Reduced quadratic programming to least squares regression - Achieved linear time and space complexities through efficient banded matrix structures&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Scalable Mechanistic Neural Networks</title>
      <link>http://localhost:1313/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces the Scalable Mechanistic Neural Network (S-MNN), an enhanced version of the original Mechanistic Neural Network (MNN) that significantly improves computational efficiency while maintaining accuracy. The key innovation is reducing both time and space complexities from cubic/quadratic to linear with respect to sequence length. &lt;strong&gt;Key Contributions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Complexity Reduction&lt;/strong&gt; - Reformulated the original MNN&amp;rsquo;s underlying linear system by eliminating slack variables and central difference constraints - Reduced quadratic programming to least squares regression - Achieved linear time and space complexities through efficient banded matrix structures&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Transcoders Find Interpretable LLM Feature Circuits</title>
      <link>http://localhost:1313/en/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces &amp;ldquo;transcoders&amp;rdquo; - a novel approach for analyzing neural network circuits in large language models (LLMs). The key innovation is that transcoders enable interpretable analysis of MLP (Multi-Layer Perceptron) layers while cleanly separating input-dependent and input-invariant behaviors. Key Points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Problem &amp;amp; Motivation&lt;/strong&gt;: Circuit analysis in LLMs aims to find interpretable subgraphs corresponding to specific behaviors. However, analyzing MLP layers is challenging because features are typically dense combinations of many neurons. Previous approaches using sparse autoencoders (SAEs) struggle to disentangle local and global behaviors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Transcoders Find Interpretable LLM Feature Circuits</title>
      <link>http://localhost:1313/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces &amp;ldquo;transcoders&amp;rdquo; - a novel approach for analyzing neural network circuits in large language models (LLMs). The key innovation is that transcoders enable interpretable analysis of MLP (Multi-Layer Perceptron) layers while cleanly separating input-dependent and input-invariant behaviors. Key Points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Problem &amp;amp; Motivation&lt;/strong&gt;: Circuit analysis in LLMs aims to find interpretable subgraphs corresponding to specific behaviors. However, analyzing MLP layers is challenging because features are typically dense combinations of many neurons. Previous approaches using sparse autoencoders (SAEs) struggle to disentangle local and global behaviors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models</title>
      <link>http://localhost:1313/en/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper investigates how large language models (LLMs) perform look-ahead planning, focusing on their internal mechanisms and ability to consider future steps when making decisions. The research provides important insights into whether LLMs plan greedily (one step at a time) or can look ahead multiple steps. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First comprehensive study of planning interpretability mechanisms in LLMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Demonstration of the &amp;ldquo;Look-Ahead Planning Decisions Existence Hypothesis&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Analysis of how internal representations encode future decisions&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models</title>
      <link>http://localhost:1313/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper investigates how large language models (LLMs) perform look-ahead planning, focusing on their internal mechanisms and ability to consider future steps when making decisions. The research provides important insights into whether LLMs plan greedily (one step at a time) or can look ahead multiple steps. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First comprehensive study of planning interpretability mechanisms in LLMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Demonstration of the &amp;ldquo;Look-Ahead Planning Decisions Existence Hypothesis&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Analysis of how internal representations encode future decisions&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using Degeneracy in the Loss Landscape for Mechanistic Interpretability</title>
      <link>http://localhost:1313/en/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper explores how degeneracy in neural networks&amp;rsquo; parameter space affects mechanistic interpretability and proposes methods to address this challenge. Here are the key points: &lt;strong&gt;Core Concept: Degeneracy in Neural Networks&lt;/strong&gt; The authors argue that a major obstacle to reverse engineering neural networks is that many parameters don&amp;rsquo;t meaningfully contribute to the network&amp;rsquo;s computation. These &amp;ldquo;degenerate&amp;rdquo; parameters can obscure the network&amp;rsquo;s internal structure. The paper identifies three main types of degeneracy:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using Degeneracy in the Loss Landscape for Mechanistic Interpretability</title>
      <link>http://localhost:1313/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/</guid>
      <description>&lt;h1&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper explores how degeneracy in neural networks&amp;rsquo; parameter space affects mechanistic interpretability and proposes methods to address this challenge. Here are the key points: &lt;strong&gt;Core Concept: Degeneracy in Neural Networks&lt;/strong&gt; The authors argue that a major obstacle to reverse engineering neural networks is that many parameters don&amp;rsquo;t meaningfully contribute to the network&amp;rsquo;s computation. These &amp;ldquo;degenerate&amp;rdquo; parameters can obscure the network&amp;rsquo;s internal structure. The paper identifies three main types of degeneracy:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;h1&gt;About AI Safety Papers&lt;/h1&gt;

&lt;p&gt;Welcome to AI Safety Papers, a curated collection of the latest research in AI safety. Our mission is to make cutting-edge AI safety research more accessible to researchers, practitioners, and enthusiasts.&lt;/p&gt;

&lt;h2&gt;What We Do&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;We aggregate recent papers from various sources, including arXiv and Semantic Scholar.&lt;/li&gt;
&lt;li&gt;Our AI-powered system summarizes these papers to provide quick insights.&lt;/li&gt;
&lt;li&gt;We present these summaries along with links to the original papers for easy access.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Why AI Safety?&lt;/h2&gt;

&lt;p&gt;As artificial intelligence continues to advance rapidly, ensuring its safe and beneficial development becomes increasingly crucial. AI safety research aims to address potential risks and challenges associated with AI systems, working towards creating AI that is aligned with human values and interests.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/en/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/en/about/</guid>
      <description>&lt;h1&gt;About AI Safety Papers&lt;/h1&gt;

&lt;p&gt;Welcome to AI Safety Papers, a curated collection of the latest research in AI safety. Our mission is to make cutting-edge AI safety research more accessible to researchers, practitioners, and enthusiasts.&lt;/p&gt;

&lt;h2&gt;What We Do&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;We aggregate recent papers from various sources, including arXiv and Semantic Scholar.&lt;/li&gt;
&lt;li&gt;Our AI-powered system summarizes these papers to provide quick insights.&lt;/li&gt;
&lt;li&gt;We present these summaries along with links to the original papers for easy access.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Why AI Safety?&lt;/h2&gt;

&lt;p&gt;As artificial intelligence continues to advance rapidly, ensuring its safe and beneficial development becomes increasingly crucial. AI safety research aims to address potential risks and challenges associated with AI systems, working towards creating AI that is aligned with human values and interests.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>