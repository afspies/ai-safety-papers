
















<!DOCTYPE html>
<html lang='en'><head>
	<meta name="generator" content="Hugo 0.136.2"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <link rel="shortcut icon" href='http://localhost:1313/favicon.ico' type="image/x-icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Safety Papers</title>

    

    

    
    <meta name="author" content="Alex Spies" />
    

    
        <meta property="og:url" content="http://localhost:1313/">
  <meta property="og:site_name" content="AI Safety Papers">
  <meta property="og:title" content="AI Safety Livefeed">
  <meta property="og:description" content="Explore our curated collection of the latest research in AI safety. Each entry represents a recent paper with a summary and link to the original source.
Holographic Quantum-Foam Blurring, and Localization of Gamma-Ray Burst GRB221009A Gamma-ray burst GRB221009A was of unprecedented brightness in gamma-rays and X-rays, and through to the far ultraviolet, allowing for identification within a host galaxy at redshift z=0.151 by multiple space and ground-based optical/near-infrared telescopes and enabling a first association - via cosmic-ray air-shower events - with a photon of 251 TeV. That is in direct tension with a potentially observable phenomenon of quantum gravity (QG), where spacetime &#34;foaminess&#34; accumulates in wavefronts propagating cosmological distances, and at high-enough energy could render distant yet bright pointlike objects invisible, by effectively spreading their photons out over the whole sky. But this effect would not result in photon loss, so it remains distinct from any absorption by extragalactic background light. A simple multiwavelength average of foam-induced blurring is described, analogous to atmospheric seeing from the ground. When scaled within the fields of view for the Fermi and Swift instruments, it fits all z&lt;5 GRB angular-resolution data of 10 MeV or any lesser peak energy and can still be consistent with the highest-energy localization of GRB221009A: a limiting bound of about 1 degree is in agreement with a holographic QG-favored formulation. Locating and Editing Factual Associations in GPT We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model&#39;s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/ Training Compute-Optimal Large Language Models We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher. Towards evaluations-based safety cases for AI scheming This report proposes three arguments that safety cases could use in relation to scheming, and sketches how evidence could be gathered from empirical evaluations, and what assumptions would need to be met to provide strong assurance. AI Safety: A Climb To Armageddon? An argument that certain AI safety measures, rather than mitigating existential risk, may instead exacerbate it is presented, forcing a re-examination of core assumptions around AI safety and points to several avenues for further research. Interpreting Attention Layer Outputs with Sparse Autoencoders The mystery of why models have so many seemingly redundant induction heads is explored, SAEs are used to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and SAEs find a sparse, interpretable decomposition are shown. Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach A set of axioms are given that formally characterize a mechanistic interpretation as a description that approximately captures the semantics of the neural network under analysis in a compositional manner and are able to reverse engineer the algorithm learned by the model. Safety Cases: How to Justify the Safety of Advanced AI Systems This work proposes a framework for organizing a safety case and discusses four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and -- if AI systems become much more powerful -- deference to credible AI advisors. Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress? A comprehensive meta-analysis of AI safety benchmarks is conducted, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety, to provide a more rigorous framework for AI safety research. The Elephant in the Room - Why AI Safety Demands Diverse Teams It is suggested that an alternative and better approach to the problem may be to treat alignment as a social science problem, since the social sciences enjoy a rich toolkit of models for understanding and aligning motivation and behavior, much of which could be repurposed to problems involving AI models. The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety This paper proposes and evaluates three models for AISI involvement and offers a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus. Bilinear MLPs enable weight-based mechanistic interpretability The results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models. Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it. Extracting Finite State Machines from Transformers Fueled by the popularity of the transformer architecture in deep learning, several works have investigated what formal languages a transformer can learn. Nonetheless, existing results remain hard to compare and a fine-grained understanding of the trainability of transformers on regular languages is still lacking. We investigate transformers trained on regular languages from a mechanistic interpretability perspective. Using an extension of the $L^*$ algorithm, we extract Moore machines from transformers. We empirically find tighter lower bounds on the trainability of transformers, when a finite number of symbols determine the state. Additionally, our mechanistic insight allows us to characterise the regular languages a one-layer transformer can learn with good length generalisation. However, we also identify failure cases where the determining symbols get misrecognised due to saturation of the attention mechanism. How to use and interpret activation patching What evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls, are focused on. Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning End-to-end (e2e) sparse dictionary learning is proposed, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders Sparse Autoencoders (SAEs) have emerged as a powerful unsupervised method for extracting sparse representations from language models, yet scalable training remains a significant challenge. We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. Modifications to a state-of-the-art SAE variant, Top-K SAEs, are evaluated across multiple dimensions. In particular, we assess the generalizability of SAEs trained on base models to longer contexts and fine-tuned models. Additionally, we analyze the geometry of learned SAE latents, confirming that \emph{feature splitting} enables the discovery of new features. The Llama Scope SAE checkpoints are publicly available at~\url{https://huggingface.co/fnlp/Llama-Scope}, alongside our scalable training, interpretation, and visualization tools at \url{https://github.com/OpenMOSS/Language-Model-SAEs}. These contributions aim to advance the open-source Sparse Autoencoder ecosystem and support mechanistic interpretability research by reducing the need for redundant SAE training. Mechanistic Interpretability for AI Safety - A Review This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding of AI systems&#39; inner workings. Mechanistic Interpretability of Reinforcement Learning Agents This paper explores the mechanistic interpretability of reinforcement learning (RL) agents through an analysis of a neural network trained on procedural maze environments. By dissecting the network&#39;s inner workings, we identified fundamental features like maze walls and pathways, forming the basis of the model&#39;s decision-making process. A significant observation was the goal misgeneralization, where the RL agent developed biases towards certain navigation strategies, such as consistently moving towards the top right corner, even in the absence of explicit goals. Using techniques like saliency mapping and feature mapping, we visualized these biases. We furthered this exploration with the development of novel tools for interactively exploring layer activations. Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations A novel hallucination mitigation method is proposed through targeted restoration of the LM&#39;s internal fact recall pipeline, demonstrating superior performance compared to baselines. Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks. Scalable Mechanistic Neural Networks We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems. Transcoders Find Interpretable LLM Feature Circuits This work introduces a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers, and suggests that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models This research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks and finding that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Using Degeneracy in the Loss Landscape for Mechanistic Interpretability Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="website">

    

    
        
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="AI Safety Livefeed">
  <meta name="twitter:description" content="Explore our curated collection of the latest research in AI safety. Each entry represents a recent paper with a summary and link to the original source.
Holographic Quantum-Foam Blurring, and Localization of Gamma-Ray Burst GRB221009A Gamma-ray burst GRB221009A was of unprecedented brightness in gamma-rays and X-rays, and through to the far ultraviolet, allowing for identification within a host galaxy at redshift z=0.151 by multiple space and ground-based optical/near-infrared telescopes and enabling a first association - via cosmic-ray air-shower events - with a photon of 251 TeV. That is in direct tension with a potentially observable phenomenon of quantum gravity (QG), where spacetime &#34;foaminess&#34; accumulates in wavefronts propagating cosmological distances, and at high-enough energy could render distant yet bright pointlike objects invisible, by effectively spreading their photons out over the whole sky. But this effect would not result in photon loss, so it remains distinct from any absorption by extragalactic background light. A simple multiwavelength average of foam-induced blurring is described, analogous to atmospheric seeing from the ground. When scaled within the fields of view for the Fermi and Swift instruments, it fits all z&lt;5 GRB angular-resolution data of 10 MeV or any lesser peak energy and can still be consistent with the highest-energy localization of GRB221009A: a limiting bound of about 1 degree is in agreement with a holographic QG-favored formulation. Locating and Editing Factual Associations in GPT We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model&#39;s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/ Training Compute-Optimal Large Language Models We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher. Towards evaluations-based safety cases for AI scheming This report proposes three arguments that safety cases could use in relation to scheming, and sketches how evidence could be gathered from empirical evaluations, and what assumptions would need to be met to provide strong assurance. AI Safety: A Climb To Armageddon? An argument that certain AI safety measures, rather than mitigating existential risk, may instead exacerbate it is presented, forcing a re-examination of core assumptions around AI safety and points to several avenues for further research. Interpreting Attention Layer Outputs with Sparse Autoencoders The mystery of why models have so many seemingly redundant induction heads is explored, SAEs are used to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and SAEs find a sparse, interpretable decomposition are shown. Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach A set of axioms are given that formally characterize a mechanistic interpretation as a description that approximately captures the semantics of the neural network under analysis in a compositional manner and are able to reverse engineer the algorithm learned by the model. Safety Cases: How to Justify the Safety of Advanced AI Systems This work proposes a framework for organizing a safety case and discusses four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and -- if AI systems become much more powerful -- deference to credible AI advisors. Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress? A comprehensive meta-analysis of AI safety benchmarks is conducted, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety, to provide a more rigorous framework for AI safety research. The Elephant in the Room - Why AI Safety Demands Diverse Teams It is suggested that an alternative and better approach to the problem may be to treat alignment as a social science problem, since the social sciences enjoy a rich toolkit of models for understanding and aligning motivation and behavior, much of which could be repurposed to problems involving AI models. The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety This paper proposes and evaluates three models for AISI involvement and offers a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus. Bilinear MLPs enable weight-based mechanistic interpretability The results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models. Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it. Extracting Finite State Machines from Transformers Fueled by the popularity of the transformer architecture in deep learning, several works have investigated what formal languages a transformer can learn. Nonetheless, existing results remain hard to compare and a fine-grained understanding of the trainability of transformers on regular languages is still lacking. We investigate transformers trained on regular languages from a mechanistic interpretability perspective. Using an extension of the $L^*$ algorithm, we extract Moore machines from transformers. We empirically find tighter lower bounds on the trainability of transformers, when a finite number of symbols determine the state. Additionally, our mechanistic insight allows us to characterise the regular languages a one-layer transformer can learn with good length generalisation. However, we also identify failure cases where the determining symbols get misrecognised due to saturation of the attention mechanism. How to use and interpret activation patching What evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls, are focused on. Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning End-to-end (e2e) sparse dictionary learning is proposed, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders Sparse Autoencoders (SAEs) have emerged as a powerful unsupervised method for extracting sparse representations from language models, yet scalable training remains a significant challenge. We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. Modifications to a state-of-the-art SAE variant, Top-K SAEs, are evaluated across multiple dimensions. In particular, we assess the generalizability of SAEs trained on base models to longer contexts and fine-tuned models. Additionally, we analyze the geometry of learned SAE latents, confirming that \emph{feature splitting} enables the discovery of new features. The Llama Scope SAE checkpoints are publicly available at~\url{https://huggingface.co/fnlp/Llama-Scope}, alongside our scalable training, interpretation, and visualization tools at \url{https://github.com/OpenMOSS/Language-Model-SAEs}. These contributions aim to advance the open-source Sparse Autoencoder ecosystem and support mechanistic interpretability research by reducing the need for redundant SAE training. Mechanistic Interpretability for AI Safety - A Review This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding of AI systems&#39; inner workings. Mechanistic Interpretability of Reinforcement Learning Agents This paper explores the mechanistic interpretability of reinforcement learning (RL) agents through an analysis of a neural network trained on procedural maze environments. By dissecting the network&#39;s inner workings, we identified fundamental features like maze walls and pathways, forming the basis of the model&#39;s decision-making process. A significant observation was the goal misgeneralization, where the RL agent developed biases towards certain navigation strategies, such as consistently moving towards the top right corner, even in the absence of explicit goals. Using techniques like saliency mapping and feature mapping, we visualized these biases. We furthered this exploration with the development of novel tools for interactively exploring layer activations. Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations A novel hallucination mitigation method is proposed through targeted restoration of the LM&#39;s internal fact recall pipeline, demonstrating superior performance compared to baselines. Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks. Scalable Mechanistic Neural Networks We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems. Transcoders Find Interpretable LLM Feature Circuits This work introduces a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers, and suggests that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models This research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks and finding that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Using Degeneracy in the Loss Landscape for Mechanistic Interpretability Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.">

    

    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                throwOnError : false
            });
        });
    </script>
    <link rel="stylesheet" href="/style.css" integrity="">





    
    <script>
        if (!('theme' in localStorage)) {
            localStorage.theme = 'light';
        }

        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.setAttribute("data-theme", "dark");
        } else {
            document.documentElement.setAttribute("data-theme", "light");
        }
    </script>
<script defer src="/js/header.js" integrity=""></script>



    <script defer src="/js/zooming.js" integrity=""></script>




    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.3/tex-mml-chtml.js"></script>
    <script defer src="/js/math.js" integrity=""></script>






    
    
    
    <script defer src="/js/search-en.js" integrity=""></script>






    
</head>
<body><header>
    <div id="header_left">
        <div id="sidebar_btn">
            <input type="checkbox" id="sidebar_btn_input" class="hidden" />
            <label id="sidebar_btn_label" for="sidebar_btn_input">
                <svg id="menu_icon" width="26px" height="26px" viewBox="0 0 24 24">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="24" height="24" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
</svg>

</svg>
            </label>
            <label id="sidebar_canvas_overlay_wrapper" for="sidebar_btn_input">
                <div id="sidebar_canvas_overlay"></div>
            </label>
            <div id="sidebar">
                <ul><li>
                            <a href="/posts/">Papers</a></li><li>
                            <a href="/about/">About</a></li></ul>
            </div>
        </div>
    
        <div class="brand">
            <div>
                <a href="/">AI Safety Papers</a>
            </div>
        </div>
    </div>

    <div class="toolbox">
        <div id="theme_tool">
            <svg id="dark_mode_btn" class="toolbox-btn" width="18px" height="18px" viewBox="0 0 24 24">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="24" height="24" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

</svg>
            <svg id="light_mode_btn" class="toolbox-btn" width="18px" height="18px" viewBox="0 0 24 24">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="24" height="24" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>

</svg>
        </div>

        
            <div id="search_tool">
                <svg id="search_btn" class="toolbox-btn" width="18px" height="18px" viewBox="0 0 24 24">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="24" height="24" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
</svg>

</svg><div id="search_menu_wrapper" class="hidden">
    <div id="search_menu">
        <div id="search_menu_toolbar">
            <div id="search_menu_input_wrapper">
                <input id="search_menu_input" type="text" placeholder='Search Posts'>
            </div>
            <div id="search_menu_close_btn">
                <svg width="18px" height="18px" viewBox="0 0 24 24">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="24" height="24" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="18" y1="6" x2="6" y2="18"></line><line x1="6" y1="6" x2="18" y2="18"></line>
</svg>

</svg>
            </div>
        </div>
        <div id="search_menu_results">
        </div>
    </div>
</div>
</div>
        

        
            <div id="translation_tool" class="dropdown-wrapper pure-menu pure-menu-horizontal toolbox-btn" onclick="void(0)">
                <ul class="pure-menu-list">
                    <li class="pure-menu-item pure-menu-has-children pure-menu-allow-hover">
                        <div class="dropdown-btn pure-menu-link">
                            <svg width="18px" height="18px" viewBox="0 0 24 24">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="24" height="24" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path>
</svg>

</svg>
                            <span class="dropdown-desc">English</span>
                        </div>
                        <ul class="pure-menu-children">
                            
                            <li class="pure-menu-item">
                                <a href="http://localhost:1313/" class="pure-menu-link">English</a>
                            </li>
                            
                            <li class="pure-menu-item">
                                <a href="http://localhost:1313/zh-tw/" class="pure-menu-link"></a>
                            </li>
                            
                        </ul>
                    </li>
                </ul>
            </div>
        
    </div>
</header>
<nav id="navbar" class="pure-menu">
    <ul class="pure-menu-list"><li class="navbar-item pure-menu-item ">
                    
                        <a href="/posts/" class="pure-menu-link">Papers</a>
                    
                </li><li class="navbar-item pure-menu-item ">
                    
                        <a href="/about/" class="pure-menu-link">About</a>
                    
                </li></ul>
</nav>
<main>
            <div id="content" class="content-margin">
                













<div class="content-margin">
    
    <div class="my-8">
        <h1>AI Safety Livefeed</h1>
        



<article >
    
    
        
        
    
    <p>Explore our curated collection of the latest research in AI safety. Each entry represents a recent paper with a summary and link to the original source.</p>














    








<div class="content-margin">
    

    
        <div class="bookcase-layout content-margin">
            
                
                



<a href="/posts/paper_07c33e0aaaa7b00c6a40bacf6a95fc3129e91e8f/" class="bookcase-item">
    <figcaption class="title-measure">
        Holographic Quantum-Foam Blurring, and Localization of Gamma-Ray Burst   GRB221009A
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_07c33e0aaaa7b00c6a40bacf6a95fc3129e91e8f/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_07c33e0aaaa7b00c6a40bacf6a95fc3129e91e8f/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                Gamma-ray burst GRB221009A was of unprecedented brightness in gamma-rays and X-rays, and through to the far ultraviolet, allowing for identification within a host galaxy at redshift z=0.151 by multiple space and ground-based optical/near-infrared telescopes and enabling a first association - via cosmic-ray air-shower events - with a photon of 251 TeV. That is in direct tension with a potentially observable phenomenon of quantum gravity (QG), where spacetime &#34;foaminess&#34; accumulates in wavefronts propagating cosmological distances, and at high-enough energy could render distant yet bright pointlike objects invisible, by effectively spreading their photons out over the whole sky. But this effect would not result in photon loss, so it remains distinct from any absorption by extragalactic background light. A simple multiwavelength average of foam-induced blurring is described, analogous to atmospheric seeing from the ground. When scaled within the fields of view for the Fermi and Swift instruments, it fits all z&lt;5 GRB angular-resolution data of 10 MeV or any lesser peak energy and can still be consistent with the highest-energy localization of GRB221009A: a limiting bound of about 1 degree is in agreement with a holographic QG-favored formulation.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_ce641b1fe35a2a64268a305380dd404e231eec02/" class="bookcase-item">
    <figcaption class="title-measure">
        Locating and Editing Factual Associations in GPT
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_ce641b1fe35a2a64268a305380dd404e231eec02/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_ce641b1fe35a2a64268a305380dd404e231eec02/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model&#39;s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_ac31afbbba5e18bbce9108f53def04efee7f486b/" class="bookcase-item">
    <figcaption class="title-measure">
        Training Compute-Optimal Large Language Models
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_ac31afbbba5e18bbce9108f53def04efee7f486b/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_ac31afbbba5e18bbce9108f53def04efee7f486b/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_be60e58e175c8940e2608aa699ded1818d83ef45/" class="bookcase-item">
    <figcaption class="title-measure">
        Towards evaluations-based safety cases for AI scheming
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_be60e58e175c8940e2608aa699ded1818d83ef45/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_be60e58e175c8940e2608aa699ded1818d83ef45/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    This report proposes three arguments that safety cases could use in relation to scheming, and sketches how evidence could be gathered from empirical evaluations, and what assumptions would need to be met to provide strong assurance.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_14d97aae72f493900b0de0044431e4f596d69120/" class="bookcase-item">
    <figcaption class="title-measure">
        AI Safety: A Climb To Armageddon?
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_14d97aae72f493900b0de0044431e4f596d69120/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_14d97aae72f493900b0de0044431e4f596d69120/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    An argument that certain AI safety measures, rather than mitigating existential risk, may instead exacerbate it is presented, forcing a re-examination of core assumptions around AI safety and points to several avenues for further research.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/" class="bookcase-item">
    <figcaption class="title-measure">
        Interpreting Attention Layer Outputs with Sparse Autoencoders
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    The mystery of why models have so many seemingly redundant induction heads is explored, SAEs are used to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and SAEs find a sparse, interpretable decomposition are shown.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_432a829987174ae1da3350c32f9b9c1d502bd0de/" class="bookcase-item">
    <figcaption class="title-measure">
        Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_432a829987174ae1da3350c32f9b9c1d502bd0de/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_432a829987174ae1da3350c32f9b9c1d502bd0de/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    A set of axioms are given that formally characterize a mechanistic interpretation as a description that approximately captures the semantics of the neural network under analysis in a compositional manner and are able to reverse engineer the algorithm learned by the model.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_26839456e694aa593a2cdea9c0442f2e8861aed3/" class="bookcase-item">
    <figcaption class="title-measure">
        Safety Cases: How to Justify the Safety of Advanced AI Systems
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_26839456e694aa593a2cdea9c0442f2e8861aed3/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_26839456e694aa593a2cdea9c0442f2e8861aed3/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    This work proposes a framework for organizing a safety case and discusses four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and -- if AI systems become much more powerful -- deference to credible AI advisors.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_50c337cb047be63614342f5d2e9e492762b7cc5c/" class="bookcase-item">
    <figcaption class="title-measure">
        Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_50c337cb047be63614342f5d2e9e492762b7cc5c/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_50c337cb047be63614342f5d2e9e492762b7cc5c/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    A comprehensive meta-analysis of AI safety benchmarks is conducted, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety, to provide a more rigorous framework for AI safety research.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_ce2bfded08db31acfb92a82625fe353fdf9549be/" class="bookcase-item">
    <figcaption class="title-measure">
        The Elephant in the Room - Why AI Safety Demands Diverse Teams
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_ce2bfded08db31acfb92a82625fe353fdf9549be/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_ce2bfded08db31acfb92a82625fe353fdf9549be/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    It is suggested that an alternative and better approach to the problem may be to treat alignment as a social science problem, since the social sciences enjoy a rich toolkit of models for understanding and aligning motivation and behavior, much of which could be repurposed to problems involving AI models.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_cf54ca4ea0e1899ddb2d0ee3e0f21c8097f44bbc/" class="bookcase-item">
    <figcaption class="title-measure">
        The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_cf54ca4ea0e1899ddb2d0ee3e0f21c8097f44bbc/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_cf54ca4ea0e1899ddb2d0ee3e0f21c8097f44bbc/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    This paper proposes and evaluates three models for AISI involvement and offers a multi-track system solution in which the central role of AISIs guarantees coherence among the different tracks and consistency in their AI safety focus.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/" class="bookcase-item">
    <figcaption class="title-measure">
        Bilinear MLPs enable weight-based mechanistic interpretability
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    The results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/" class="bookcase-item">
    <figcaption class="title-measure">
        Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/" class="bookcase-item">
    <figcaption class="title-measure">
        Extracting Finite State Machines from Transformers
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                Fueled by the popularity of the transformer architecture in deep learning, several works have investigated what formal languages a transformer can learn. Nonetheless, existing results remain hard to compare and a fine-grained understanding of the trainability of transformers on regular languages is still lacking. We investigate transformers trained on regular languages from a mechanistic interpretability perspective. Using an extension of the $L^*$ algorithm, we extract Moore machines from transformers. We empirically find tighter lower bounds on the trainability of transformers, when a finite number of symbols determine the state. Additionally, our mechanistic insight allows us to characterise the regular languages a one-layer transformer can learn with good length generalisation. However, we also identify failure cases where the determining symbols get misrecognised due to saturation of the attention mechanism.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/" class="bookcase-item">
    <figcaption class="title-measure">
        How to use and interpret activation patching
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    What evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls, are focused on.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/" class="bookcase-item">
    <figcaption class="title-measure">
        Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    End-to-end (e2e) sparse dictionary learning is proposed, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/" class="bookcase-item">
    <figcaption class="title-measure">
        Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                Sparse Autoencoders (SAEs) have emerged as a powerful unsupervised method for extracting sparse representations from language models, yet scalable training remains a significant challenge. We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. Modifications to a state-of-the-art SAE variant, Top-K SAEs, are evaluated across multiple dimensions. In particular, we assess the generalizability of SAEs trained on base models to longer contexts and fine-tuned models. Additionally, we analyze the geometry of learned SAE latents, confirming that \emph{feature splitting} enables the discovery of new features. The Llama Scope SAE checkpoints are publicly available at~\url{https://huggingface.co/fnlp/Llama-Scope}, alongside our scalable training, interpretation, and visualization tools at \url{https://github.com/OpenMOSS/Language-Model-SAEs}. These contributions aim to advance the open-source Sparse Autoencoder ecosystem and support mechanistic interpretability research by reducing the need for redundant SAE training.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/" class="bookcase-item">
    <figcaption class="title-measure">
        Mechanistic Interpretability for AI Safety - A Review
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding of AI systems&#39; inner workings.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/" class="bookcase-item">
    <figcaption class="title-measure">
        Mechanistic Interpretability of Reinforcement Learning Agents
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                This paper explores the mechanistic interpretability of reinforcement learning (RL) agents through an analysis of a neural network trained on procedural maze environments. By dissecting the network&#39;s inner workings, we identified fundamental features like maze walls and pathways, forming the basis of the model&#39;s decision-making process. A significant observation was the goal misgeneralization, where the RL agent developed biases towards certain navigation strategies, such as consistently moving towards the top right corner, even in the absence of explicit goals. Using techniques like saliency mapping and feature mapping, we visualized these biases. We furthered this exploration with the development of novel tools for interactively exploring layer activations.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/" class="bookcase-item">
    <figcaption class="title-measure">
        Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    A novel hallucination mitigation method is proposed through targeted restoration of the LM&#39;s internal fact recall pipeline, demonstrating superior performance compared to baselines.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/" class="bookcase-item">
    <figcaption class="title-measure">
        Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/" class="bookcase-item">
    <figcaption class="title-measure">
        Scalable Mechanistic Neural Networks
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/" class="bookcase-item">
    <figcaption class="title-measure">
        Transcoders Find Interpretable LLM Feature Circuits
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    This work introduces a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers, and suggests that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/" class="bookcase-item">
    <figcaption class="title-measure">
        Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                
                    This research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks and finding that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent.
                
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
                
                



<a href="/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/" class="bookcase-item">
    <figcaption class="title-measure">
        Using Degeneracy in the Loss Landscape for Mechanistic Interpretability
    </figcaption>
    <figure>
        <img src="http://localhost:1313/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/thumbnail.png" class="bookcase-item-img-light">
        <img src="http://localhost:1313/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/thumbnail.png" class="bookcase-item-img-dark">
    </figure>
    <div class="bookcase-item-hover">
        <div class="hover-text">
            
                Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.
            
        </div>
    </div>
</a>

<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.bookcase-item').forEach(item => {
        const title = item.querySelector('.title-measure');
        const hover = item.querySelector('.bookcase-item-hover');
        hover.style.setProperty('--title-height', title.offsetHeight + 'px');
    });
});
</script>

            
        </div>
    
</div>

</article>

    </div>
    

    
</div>



                
                    
                
            </div>
        </main>
<footer>
    <article>Copyright © 2024 AI Safety Papers</article>
</footer>

</body>
</html>
