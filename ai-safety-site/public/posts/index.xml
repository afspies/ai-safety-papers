<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on AI Safety Papers</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on AI Safety Papers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 14 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Holographic Quantum-Foam Blurring, and Localization of Gamma-Ray Burst   GRB221009A</title>
      <link>http://localhost:1313/posts/paper_07c33e0aaaa7b00c6a40bacf6a95fc3129e91e8f/</link>
      <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_07c33e0aaaa7b00c6a40bacf6a95fc3129e91e8f/</guid>
      <description>&lt;div class=&#34;paper-meta&#34;&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Authors:&lt;/span&gt;
    &lt;div class=&#34;paper-authors&#34;&gt;
      Eric Steinbring
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Published:&lt;/span&gt;
    &lt;span&gt;Unknown&lt;/span&gt;
  &lt;/div&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Original Paper:&lt;/span&gt;
    &lt;a href=&#34;https://arxiv.org/abs/2311.10168&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View Paper&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper discusses Holographic Quantum-Foam Blurring, and Localization of Gamma-Ray Burst   GRB221009A.&lt;/p&gt;
&lt;p&gt;The authors propose a novel approach to address important challenges in the field.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Angular localization (or equivalent wavefront phase dispersion) of GRB221009A: Fermi LAT and GBM; Swift BAT, XRT and UVOT; or other space- and ground-based facilities. FoVs and energy-sensitivity ranges of instruments are indicated by thin black outlines, and scaled LAT PSF is that measured from AGNs (dot-dashed curve; see Appendix A); gray-shaded regions are instrument resolutions, with increasingly dark shading indicating diffraction: Fermi LAT ( $D=1.8~{}{\rm m}$ ), darker, HST ( $D=2.4~{}{\rm m}$ ) and darkest JWST ( $D=6.5~{}{\rm m}$ ). Maximal limits imposed by foam-induced blurring (orange-shaded regions; $\alpha=0.650$ and $z=4.61$ ) and combination with instrument viewing angles (blue) are shown; their average $\Phi$ (gray curves, $\alpha=0.667$ and $z=1.41$ ) and minima (thick white curve above Theta, thick black below; $\alpha=0.735$ and $z=0.151$ ) follows in Section 2; apparent source sizes (dashed horizontal lines, calculated for $z=0.151$ ) and localization/resolution (horizontal dot-dashed); for details of all other available data, which are shown here as density contours, see Section 3.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Angular localization (or equivalent wavefront phase dispersion) of GRB221009A: Fermi LAT and GBM; Swift BAT, XRT and UVOT; or other space- and ground-based facilities. FoVs and energy-sensitivity ranges of instruments are indicated by thin black outlines, and scaled LAT PSF is that measured from AGNs (dot-dashed curve; see Appendix A); gray-shaded regions are instrument resolutions, with increasingly dark shading indicating diffraction: Fermi LAT ( $D=1.8~{}{\rm m}$ ), darker, HST ( $D=2.4~{}{\rm m}$ ) and darkest JWST ( $D=6.5~{}{\rm m}$ ). Maximal limits imposed by foam-induced blurring (orange-shaded regions; $\alpha=0.650$ and $z=4.61$ ) and combination with instrument viewing angles (blue) are shown; their average $\Phi$ (gray curves, $\alpha=0.667$ and $z=1.41$ ) and minima (thick white curve above Theta, thick black below; $\alpha=0.735$ and $z=0.151$ ) follows in Section 2; apparent source sizes (dashed horizontal lines, calculated for $z=0.151$ ) and localization/resolution (horizontal dot-dashed); for details of all other available data, which are shown here as density contours, see Section 3.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
Figure 1 shows the main architecture of their proposed method.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Locating and Editing Factual Associations in GPT</title>
      <link>http://localhost:1313/posts/paper_ce641b1fe35a2a64268a305380dd404e231eec02/</link>
      <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_ce641b1fe35a2a64268a305380dd404e231eec02/</guid>
      <description>&lt;div class=&#34;paper-meta&#34;&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Authors:&lt;/span&gt;
    &lt;div class=&#34;paper-authors&#34;&gt;
      Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Published:&lt;/span&gt;
    &lt;span&gt;Unknown&lt;/span&gt;
  &lt;/div&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Original Paper:&lt;/span&gt;
    &lt;a href=&#34;https://arxiv.org/abs/2202.05262&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View Paper&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;paper-summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Paper Summary
  &lt;a href=&#34;#paper-summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents a novel method for understanding and editing factual knowledge in GPT language models through targeted interventions in middle-layer MLPs. The authors combine causal tracing analysis with a rank-one editing technique (ROME) to identify and modify specific network components responsible for storing factual associations, addressing the challenge of precisely editing model knowledge while maintaining model integrity.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Training Compute-Optimal Large Language Models</title>
      <link>http://localhost:1313/posts/paper_ac31afbbba5e18bbce9108f53def04efee7f486b/</link>
      <pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_ac31afbbba5e18bbce9108f53def04efee7f486b/</guid>
      <description>&lt;div class=&#34;paper-meta&#34;&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Authors:&lt;/span&gt;
    &lt;div class=&#34;paper-authors&#34;&gt;
      Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Published:&lt;/span&gt;
    &lt;span&gt;Unknown&lt;/span&gt;
  &lt;/div&gt;
  &lt;div class=&#34;paper-meta-item&#34;&gt;
    &lt;span class=&#34;paper-meta-label&#34;&gt;Original Paper:&lt;/span&gt;
    &lt;a href=&#34;https://arxiv.org/abs/2203.15556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View Paper&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper discusses Training Compute-Optimal Large Language Models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Towards evaluations-based safety cases for AI scheming</title>
      <link>http://localhost:1313/posts/paper_be60e58e175c8940e2608aa699ded1818d83ef45/</link>
      <pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_be60e58e175c8940e2608aa699ded1818d83ef45/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents a framework for developing safety cases to address the risk of AI systems engaging in &amp;ldquo;scheming&amp;rdquo; - the covert pursuit of misaligned goals. The authors, primarily from Apollo Research and other institutions, propose structured arguments and evaluation methods to demonstrate that AI systems are unlikely to cause catastrophic outcomes through scheming behavior.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AI Safety: A Climb To Armageddon?</title>
      <link>http://localhost:1313/posts/paper_14d97aae72f493900b0de0044431e4f596d69120/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_14d97aae72f493900b0de0044431e4f596d69120/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This fascinating paper presents a counterintuitive argument against AI safety measures, suggesting that such measures might actually increase rather than decrease existential risks from AI. The authors (Cappelen, Dever, and Hawthorne) develop what they call the &amp;ldquo;non-deterministic argument&amp;rdquo; through an analogy with rock climbing.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Interpreting Attention Layer Outputs with Sparse Autoencoders</title>
      <link>http://localhost:1313/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_efbe48e4c17a097f4feaec8cf9dec40218a27a87/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents a novel approach to interpreting attention mechanisms in transformer models using Sparse Autoencoders (SAEs). The authors demonstrate that SAEs can effectively decompose attention layer outputs into interpretable features, providing new insights into how attention layers process information.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;Overview. We train Sparse Autoencoders (SAEs) on $\textbf{z}_{\text{cat}}$ , the attention layer outputs pre-linear, concatenated across all heads. The SAEs extract linear directions that correspond to concepts in the model, giving us insight into what attention layers learn in practice. Further, we uncover what information was used to compute these features with direct feature attribution (DFA, Section 2).&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;Overview. We train Sparse Autoencoders (SAEs) on $\textbf{z}_{\text{cat}}$ , the attention layer outputs pre-linear, concatenated across all heads. The SAEs extract linear directions that correspond to concepts in the model, giving us insight into what attention layers learn in practice. Further, we uncover what information was used to compute these features with direct feature attribution (DFA, Section 2).&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
Figure 1&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistically Interpreting a Transformer-based 2-SAT Solver: An Axiomatic Approach</title>
      <link>http://localhost:1313/posts/paper_432a829987174ae1da3350c32f9b9c1d502bd0de/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_432a829987174ae1da3350c32f9b9c1d502bd0de/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents an important theoretical framework for mechanistic interpretability of neural networks, introducing formal axioms to characterize what constitutes a valid mechanistic interpretation. The authors demonstrate their approach through a detailed case study of a Transformer model trained to solve 2-SAT problems.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Safety Cases: How to Justify the Safety of Advanced AI Systems</title>
      <link>http://localhost:1313/posts/paper_26839456e694aa593a2cdea9c0442f2e8861aed3/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_26839456e694aa593a2cdea9c0442f2e8861aed3/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This comprehensive paper presents a framework for developing and evaluating safety cases for advanced AI systems. The authors propose a structured approach to justify that AI systems are unlikely to cause catastrophic outcomes during deployment.&lt;/p&gt;
&lt;p&gt;Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Framework Structure
The paper outlines a six-step framework for constructing AI safety cases:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Define the AI macrosystem and deployment decision&lt;/li&gt;
&lt;li&gt;Specify unacceptable outcomes&lt;/li&gt;
&lt;li&gt;Justify deployment assumptions&lt;/li&gt;
&lt;li&gt;Decompose the macrosystem into subsystems&lt;/li&gt;
&lt;li&gt;Assess subsystem risk&lt;/li&gt;
&lt;li&gt;Assess macrosystem risk&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Safety Argument Categories
The authors identify four main categories of safety arguments (
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig2.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; As AI systems become more powerful, developers will likely increasingly rely on arguments toward the right in the plot above.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig2.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; As AI systems become more powerful, developers will likely increasingly rely on arguments toward the right in the plot above.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
Figure 2):&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Inability: Demonstrating AI systems cannot cause catastrophes&lt;/li&gt;
&lt;li&gt;Control: Showing control measures prevent catastrophic outcomes&lt;/li&gt;
&lt;li&gt;Trustworthiness: Proving AI systems behave reliably despite capabilities&lt;/li&gt;
&lt;li&gt;Deference: Relying on credible AI advisors&amp;rsquo; safety assessments&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Building Block Arguments
The paper presents a detailed analysis of specific arguments within each category (
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig3.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; building block arguments for making safety cases. See section 5 for detailed descriptions of each building block argument and justifications for their labels. The arguments that rank highest on practicality, strength, and scalability are monitoring, externalized reasoning, and testbeds.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig3.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; building block arguments for making safety cases. See section 5 for detailed descriptions of each building block argument and justifications for their labels. The arguments that rank highest on practicality, strength, and scalability are monitoring, externalized reasoning, and testbeds.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
Figure 3), rating them on:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Practicality: How soon they could be implemented&lt;/li&gt;
&lt;li&gt;Maximum strength: Potential confidence in safety&lt;/li&gt;
&lt;li&gt;Scalability: Applicability to more advanced AI systems&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;Holistic Safety Case Construction
The authors demonstrate how to combine these arguments into a complete safety case using Goal Structuring Notation (GSN) (
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig8.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; Caption: The diagram above illustrates an example monitoring scheme where a watchdog model (right) monitors the agent model (left). In this setup, a control argument could be constructed by conservatively assuming that the agent and its users are pursuing their best strategy for causing a catastrophe (untrustworthy). The watchdog is assumed to be reliable, which must be justified with a trustworthiness argument.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig8.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; Caption: The diagram above illustrates an example monitoring scheme where a watchdog model (right) monitors the agent model (left). In this setup, a control argument could be constructed by conservatively assuming that the agent and its users are pursuing their best strategy for causing a catastrophe (untrustworthy). The watchdog is assumed to be reliable, which must be justified with a trustworthiness argument.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
Figure 8,
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig9.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 9:&lt;/strong&gt; the diagram above depicts a potential setup where three different models would need to behave badly to cause a catastrophe.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig9.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 9:&lt;/strong&gt; the diagram above depicts a potential setup where three different models would need to behave badly to cause a catastrophe.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
Figure 9).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?</title>
      <link>http://localhost:1313/posts/paper_50c337cb047be63614342f5d2e9e492762b7cc5c/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_50c337cb047be63614342f5d2e9e492762b7cc5c/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This comprehensive paper examines the relationship between AI safety benchmarks and general model capabilities, introducing the concept of &amp;ldquo;safetywashing&amp;rdquo; - where capability improvements are misrepresented as safety advancements. The authors conduct an extensive meta-analysis across multiple safety domains and dozens of models to determine whether common safety benchmarks actually measure properties distinct from general capabilities.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Elephant in the Room - Why AI Safety Demands Diverse Teams</title>
      <link>http://localhost:1313/posts/paper_ce2bfded08db31acfb92a82625fe353fdf9549be/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_ce2bfded08db31acfb92a82625fe353fdf9549be/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper, &amp;ldquo;The Elephant in the Room - Why AI Safety Demands Diverse Teams&amp;rdquo; by David Rostcheck and Lara Scheibling, presents a compelling argument for treating AI alignment as a social science problem rather than purely a technical one. Here are the key points:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Role of AI Safety Institutes in Contributing to International Standards for Frontier AI Safety</title>
      <link>http://localhost:1313/posts/paper_cf54ca4ea0e1899ddb2d0ee3e0f21c8097f44bbc/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_cf54ca4ea0e1899ddb2d0ee3e0f21c8097f44bbc/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper examines the potential role of AI Safety Institutes (AISIs) in developing international standards for frontier AI safety. The authors propose three distinct but complementary models for AISI involvement in standard-setting processes, analyzing their respective strengths and limitations.&lt;/p&gt;
&lt;p&gt;Key Points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Context and Motivation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;International standards are crucial for ensuring safe development and deployment of frontier AI systems&lt;/li&gt;
&lt;li&gt;AISIs are well-positioned to contribute due to their technical expertise, international engagement mandate, and convening power&lt;/li&gt;
&lt;li&gt;Current standardization processes face challenges in keeping pace with rapid AI development&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Three Proposed Models&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The paper presents three models for AISI involvement in standard-setting:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bilinear MLPs enable weight-based mechanistic interpretability</title>
      <link>http://localhost:1313/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_1ac2d6f203b3bdd3503357b0839c24a485d6ce1d/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces an important advancement in neural network interpretability by analyzing bilinear MLPs, which are a variant of Gated Linear Units (GLUs) without element-wise nonlinearity. The authors demonstrate that these bilinear MLPs can achieve competitive performance while being significantly more interpretable than traditional MLPs. Key contributions:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT</title>
      <link>http://localhost:1313/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_1301ed763095097ff424c668e16a265b3ae2f231/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents a novel approach to discovering interpretable circuits in transformer models using dictionary learning, focusing on a case study of Othello-GPT. The work advances mechanistic interpretability by providing a patch-free alternative to traditional circuit discovery methods. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A new framework for circuit discovery that avoids activation patching and offers better computational efficiency&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Extracting Finite State Machines from Transformers</title>
      <link>http://localhost:1313/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_914ec7e316370cc23ae5cbd0031960151de4e43d/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper investigates how transformers learn regular languages from a mechanistic interpretability perspective. The authors develop a method to extract finite state machines (Moore machines) from trained transformers and analyze how these models internally represent different states. Key contributions: - Extension of the L* algorithm to extract Moore machines from transformers - Analysis of how transformers represent states in their activation space - Characterization of which regular languages one-layer transformers can learn with good generalization - Identification of failure modes due to attention saturation The authors begin by examining different training tasks for regular languages:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to use and interpret activation patching</title>
      <link>http://localhost:1313/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_a0b775b9ff82ce1fb7dd34d53a7d09f70b171895/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper provides a comprehensive guide to activation patching, an important mechanistic interpretability technique for understanding neural networks. The authors, Stefan Heimersheim and Neel Nanda, draw from extensive practical experience to outline best practices and common pitfalls. Key aspects covered include: **&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Core Concepts** - Activation patching involves replacing internal activations of a neural network during execution to understand how different components contribute to model behavior - The technique is more targeted than simple ablation, allowing researchers to isolate specific behaviors and circuits - The authors illustrate the concept with an example of analyzing how a model completes &amp;ldquo;The Colosseum is in&amp;rdquo; â†’ &amp;ldquo;Rome&amp;rdquo;, demonstrating how different patching approaches can isolate language processing vs factual recall **&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning</title>
      <link>http://localhost:1313/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_02ad427b0d20fb976741e332f69c2fd00c751164/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces end-to-end sparse dictionary learning (e2e SAE), a novel method for training sparse autoencoders (SAEs) that improves their ability to identify functionally important features in neural networks. &lt;strong&gt;Key Innovation:&lt;/strong&gt; Traditional SAEs are trained to minimize reconstruction error of network activations, but this approach may learn features that aren&amp;rsquo;t maximally relevant for the network&amp;rsquo;s actual computation. The authors propose training SAEs by minimizing the KL divergence between the output distributions of:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders</title>
      <link>http://localhost:1313/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_2454c15f9708dc337a2ed849e897f99c43b8f6cb/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces Llama Scope, a comprehensive suite of 256 Sparse Autoencoders (SAEs) trained on the Llama-3.1-8B language model. The work represents a significant advancement in mechanistic interpretability research by providing ready-to-use, open-source SAE models that can help researchers understand the internal representations of large language models. Key aspects of the work include:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Interpretability for AI Safety - A Review</title>
      <link>http://localhost:1313/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_8b750488d139f9beba0815ff8f46ebe15ebb3e58/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This comprehensive review paper examines mechanistic interpretability, an emerging approach to understanding the inner workings of AI systems that aims to reverse engineer neural networks into human-understandable algorithms and concepts.&lt;/p&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
&lt;p&gt;Figure 1 The paper begins by contrasting mechanistic interpretability with other interpretability paradigms: - Behavioral: Analyzes input-output relationships - Attributional: Quantifies individual input feature influences - Concept-based: Identifies high-level representations - Mechanistic: Uncovers precise causal mechanisms from inputs to outputs The authors establish several core concepts and hypotheses:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Interpretability of Reinforcement Learning Agents</title>
      <link>http://localhost:1313/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_a800bac1609408eb955625b7ce0df234d48d3845/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper explores the mechanistic interpretability of reinforcement learning (RL) agents by analyzing a neural network trained on procedural maze environments. The research focuses on understanding how the network processes information and makes decisions, with particular attention to goal misgeneralization. Key aspects of the study:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations</title>
      <link>http://localhost:1313/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_01b4977937966694c00f6c7b55e712eef50603a4/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents a comprehensive analysis of non-factual hallucinations in large language models (LMs), identifying two distinct mechanisms through which these errors occur and proposing a novel method for mitigating them. Key Contributions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identification of Two Hallucination Types:&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&#34;paper-figure&#34;&gt;
  &lt;div class=&#34;figure-wrapper&#34;&gt;
    &lt;div class=&#34;figure-container&#34; onclick=&#34;toggleExpand(this)&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Our main finding of two non-factual hallucination mechanisms. Left (a): The early-site hallucinations are caused by lacking general knowledge of the subject in lower layer MLPs of transformer LMs â€“ in this case, the model fails to retrieve useful information about the entity (e.g., Orica, an Australian-based multinational corporation) to generate the correct object attribute (e.g., Australia), and therefore outputs a highly irrelevant prediction (January). Right (b): The late-site hallucinations are caused by the failure of upper layer attention heads and MLPs to identify the most relevant object to the given subject and relation â€“ in this case, the model is able to retrieve related information about the subject (e.g., Toulouse, a French city) from lower layer MLPs, but cannot distinguish the irrelevant yet strongly associated attributes (e.g., Paris) from the correct answers (e.g., Bologna/Chongqing/Atlanta). We found that these two types of hallucinations can be distinguished by the relative causal contribution to model predictions between lower and upper layer LM components.&lt;/figcaption&gt;
      
    &lt;/div&gt;
    
    &lt;div class=&#34;static-clone&#34;&gt;
      &lt;div class=&#34;image-wrapper&#34;&gt;
        &lt;img src=&#34;fig1.png&#34; &gt;
      &lt;/div&gt;
      
      &lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Our main finding of two non-factual hallucination mechanisms. Left (a): The early-site hallucinations are caused by lacking general knowledge of the subject in lower layer MLPs of transformer LMs â€“ in this case, the model fails to retrieve useful information about the entity (e.g., Orica, an Australian-based multinational corporation) to generate the correct object attribute (e.g., Australia), and therefore outputs a highly irrelevant prediction (January). Right (b): The late-site hallucinations are caused by the failure of upper layer attention heads and MLPs to identify the most relevant object to the given subject and relation â€“ in this case, the model is able to retrieve related information about the subject (e.g., Toulouse, a French city) from lower layer MLPs, but cannot distinguish the irrelevant yet strongly associated attributes (e.g., Paris) from the correct answers (e.g., Bologna/Chongqing/Atlanta). We found that these two types of hallucinations can be distinguished by the relative causal contribution to model predictions between lower and upper layer LM components.&lt;/figcaption&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;script&gt;
function toggleExpand(element) {
    const figure = element.closest(&#39;.paper-figure&#39;);
    const body = document.body;
    
    
    if (figure.classList.contains(&#39;subfigure-content&#39;)) {
        return;
    }
    
    figure.classList.toggle(&#39;expanded&#39;);
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        body.classList.add(&#39;has-expanded-figure&#39;);
    } else {
        body.classList.remove(&#39;has-expanded-figure&#39;);
    }
    
    
    const handleEscape = (e) =&gt; {
        if (e.key === &#39;Escape&#39; &amp;&amp; figure.classList.contains(&#39;expanded&#39;)) {
            toggleExpand(element);
            document.removeEventListener(&#39;keydown&#39;, handleEscape);
        }
    };
    
    
    const handleClickOutside = (e) =&gt; {
        if (figure.classList.contains(&#39;expanded&#39;) &amp;&amp; !element.contains(e.target)) {
            toggleExpand(element);
            document.removeEventListener(&#39;click&#39;, handleClickOutside);
        }
    };
    
    if (figure.classList.contains(&#39;expanded&#39;)) {
        document.addEventListener(&#39;keydown&#39;, handleEscape);
        setTimeout(() =&gt; {
            document.addEventListener(&#39;click&#39;, handleClickOutside);
        }, 100);
    }
}


document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {
    const subfigureGroups = document.querySelectorAll(&#39;.subfigures&#39;);
    subfigureGroups.forEach(group =&gt; {
        group.addEventListener(&#39;click&#39;, (e) =&gt; {
            if (e.target.closest(&#39;.subfigure-content&#39;)) {
                e.preventDefault();
                e.stopPropagation();
                
                
                const expandedView = document.createElement(&#39;div&#39;);
                expandedView.className = &#39;subfigures-expanded&#39;;
                expandedView.innerHTML = group.innerHTML;
                
                
                const closeBtn = document.createElement(&#39;button&#39;);
                closeBtn.className = &#39;close-expanded&#39;;
                closeBtn.innerHTML = &#39;Ã—&#39;;
                closeBtn.onclick = () =&gt; {
                    expandedView.remove();
                    document.body.classList.remove(&#39;has-expanded-figure&#39;);
                };
                expandedView.appendChild(closeBtn);
                
                document.body.appendChild(expandedView);
                document.body.classList.add(&#39;has-expanded-figure&#39;);
                
                
                const handleEscape = (e) =&gt; {
                    if (e.key === &#39;Escape&#39;) {
                        expandedView.remove();
                        document.body.classList.remove(&#39;has-expanded-figure&#39;);
                        document.removeEventListener(&#39;keydown&#39;, handleEscape);
                    }
                };
                document.addEventListener(&#39;keydown&#39;, handleEscape);
            }
        });
    });
});
&lt;/script&gt; 
&lt;p&gt;Figure 1 - Knowledge Enrichment Hallucinations: Occur when lower-layer MLPs fail to retrieve sufficient subject attribute knowledge - Answer Extraction Hallucinations: Happen when upper-layer attention heads fail to select the correct object attribute&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization</title>
      <link>http://localhost:1313/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_23a921483746b8b3c828bd601f54d485bec32014/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper presents &amp;ldquo;mechanistic unlearning,&amp;rdquo; a novel approach for robustly removing or editing knowledge in large language models (LLMs) by targeting specific model components identified through mechanistic interpretability. Key Contributions: - Introduces a more precise and effective method for knowledge editing/unlearning that focuses on modifying the &amp;ldquo;fact lookup&amp;rdquo; (FLU) mechanism rather than just output-focused components - Demonstrates superior robustness against prompt variations and relearning attempts compared to existing methods - Provides evidence that targeting FLU mechanisms leads to better disruption of latent knowledge while maintaining general model capabilities&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Scalable Mechanistic Neural Networks</title>
      <link>http://localhost:1313/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_8d42985327b6d112134d96cdd6def5ff9edee92a/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces the Scalable Mechanistic Neural Network (S-MNN), an enhanced version of the original Mechanistic Neural Network (MNN) that significantly improves computational efficiency while maintaining accuracy. The key innovation is reducing both time and space complexities from cubic/quadratic to linear with respect to sequence length. &lt;strong&gt;Key Contributions:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Transcoders Find Interpretable LLM Feature Circuits</title>
      <link>http://localhost:1313/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_3c6da6f1601aee99b8e5b8dcf2d21c42d9252b04/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper introduces &amp;ldquo;transcoders&amp;rdquo; - a novel approach for analyzing neural network circuits in large language models (LLMs). The key innovation is that transcoders enable interpretable analysis of MLP (Multi-Layer Perceptron) layers while cleanly separating input-dependent and input-invariant behaviors. Key Points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Problem &amp;amp; Motivation&lt;/strong&gt;: Circuit analysis in LLMs aims to find interpretable subgraphs corresponding to specific behaviors. However, analyzing MLP layers is challenging because features are typically dense combinations of many neurons. Previous approaches using sparse autoencoders (SAEs) struggle to disentangle local and global behaviors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models</title>
      <link>http://localhost:1313/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_cfbdf67fc11977637d4cb13ed7e1abce75623796/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper investigates how large language models (LLMs) perform look-ahead planning, focusing on their internal mechanisms and ability to consider future steps when making decisions. The research provides important insights into whether LLMs plan greedily (one step at a time) or can look ahead multiple steps. Key Contributions:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using Degeneracy in the Loss Landscape for Mechanistic Interpretability</title>
      <link>http://localhost:1313/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paper_c1bc03a045ea830894fe3b1799928c9f8c14923c/</guid>
      <description>&lt;h1 id=&#34;summary&#34; class=&#34;header-anchor-wrapper&#34;&gt;Summary
  &lt;a href=&#34;#summary&#34; class=&#34;header-anchor-link&#34;&gt;
    &lt;svg width=&#34;16px&#34; height=&#34;16px&#34; viewBox=&#34;0 0 24 24&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/svg&gt;
  &lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;This paper explores how degeneracy in neural networks&amp;rsquo; parameter space affects mechanistic interpretability and proposes methods to address this challenge. Here are the key points: &lt;strong&gt;Core Concept: Degeneracy in Neural Networks&lt;/strong&gt; The authors argue that a major obstacle to reverse engineering neural networks is that many parameters don&amp;rsquo;t meaningfully contribute to the network&amp;rsquo;s computation. These &amp;ldquo;degenerate&amp;rdquo; parameters can obscure the network&amp;rsquo;s internal structure. The paper identifies three main types of degeneracy:&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>