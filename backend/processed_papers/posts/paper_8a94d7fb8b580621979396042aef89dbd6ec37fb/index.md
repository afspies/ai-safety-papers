---
title: "Open Problems in Mechanistic Interpretability"
description: "Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this..."
authors: ["Lee Sharkey", "Bilal Chughtai", "Joshua Batson", "Jack Lindsey", "Jeff Wu", "Lucius Bushnaq", "Nicholas Goldowsky-Dill", "Stefan Heimersheim", "Alejandro Ortega", "Joseph Bloom", "Stella Biderman", "Adri√† Garriga-Alonso", "Arthur Conmy", "Neel Nanda", "Jessica Rumbelow", "Martin Wattenberg", "Nandi Schoots", "Joseph Miller", "Eric J. Michaud", "Stephen Casper", "Max Tegmark", "William Saunders", "David Bau", "Eric Todd", "Atticus Geiger", "Mor Geva", "Jesse Hoogland", "Daniel Murfet", "Thomas McGrath"]
date: 2025-03-28
paper_url: "https://arxiv.org/pdf/2501.16496.pdf"
---

# Open Problems in Mechanistic Interpretability

## Core Insight
Mechanistic interpretability aims to understand neural networks' computational mechanisms to provide greater assurance over AI behavior and advance scientific understanding of intelligence, but faces numerous open problems across methods, applications, and socio-technical domains that must be solved before its benefits can be fully realized **Figure 6**.

## Key Contributions

## Figures

![6](https://assets.afspies.com/figures/8a94d7fb8b580621979396042aef89dbd6ec37fb/fig6.png)

